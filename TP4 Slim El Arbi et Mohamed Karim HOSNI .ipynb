{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## TP4 Machine learning \n",
        "réalisé par : Mohamed Karim HOSNI et Slim El Arbi "
      ],
      "metadata": {
        "id": "o1Wa5ZF_swWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "import numpy as np\n",
        "import urllib.request\n",
        "import pandas as pd\n",
        "pd.options.mode.chained_assignment = None  \n",
        "from io import StringIO\n"
      ],
      "metadata": {
        "id": "m8FgEtzIdNfC"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from datetime import timedelta\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "metadata": {
        "id": "imbkK3aYnZAE"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_melbourne_data() -> pd.DataFrame:\n",
        "  \n",
        "    raw_url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/daily-min-temperatures.csv\" \n",
        "    response = urllib.request.urlopen(raw_url)\n",
        "    response = response.read().decode('utf-8')\n",
        "    \n",
        "    data = pd.read_csv(StringIO(response))\n",
        "    data['Date'] = pd.to_datetime(data['Date'])\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def split_train_test_data(melbourne_data: pd.DataFrame, split_year: str=\"1987\") -> (pd.DataFrame, pd.DataFrame):\n",
        "\n",
        "    split_year = \"{}\".format(int(split_year) - 1)\n",
        "\n",
        "    train_data = melbourne_data.loc[:split_year]\n",
        "\n",
        "    test_data = melbourne_data.loc[split_year:]\n",
        "\n",
        "    return train_data, test_data\n",
        "\n",
        "def build_training_point(data, t_str, history_days=64, horizon_days=1):\n",
        "    \n",
        "    t_datetime = datetime.strptime(t_str, \"%Y-%m-%d 00:00:00\")\n",
        "\n",
        "    try:\n",
        "        x = data.loc[t_datetime - timedelta(days=history_days - 1):t_datetime]\n",
        "        y = data.loc[t_datetime + timedelta(days=1):t_datetime + timedelta(days=horizon_days)]\n",
        "    except KeyError:\n",
        "        raise KeyError(\"The date {} is not in the data\".format(t_str))\n",
        "\n",
        "    # Return\n",
        "    return x, y\n",
        "\n",
        "\n",
        "def create_training_points(data, history_days=64, horizon_days=32):\n",
        "\n",
        "    X = []\n",
        "    Y = []\n",
        "    for t in data.index[history_days:(len(data) - horizon_days)]:\n",
        "        try:\n",
        "            x, y = build_training_point(data, str(t), history_days=history_days, horizon_days=horizon_days)\n",
        "            if (len(x) == history_days) & (len(y) == horizon_days):\n",
        "                X.append(x)\n",
        "                Y.append(y)\n",
        "        except KeyError:\n",
        "            continue\n",
        "    X = np.stack(X)\n",
        "    Y = np.stack(Y)\n",
        "    return X, Y\n",
        "\n",
        "melbourne_data = get_melbourne_data()"
      ],
      "metadata": {
        "id": "qrKtDXYldXpR"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(melbourne_data.head())"
      ],
      "metadata": {
        "id": "6Kk1q-m6eyfZ",
        "outputId": "d58bca35-509a-440a-c551-d0a5b3c0c3f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        Date  Temp\n",
            "0 1981-01-01  20.7\n",
            "1 1981-01-02  17.9\n",
            "2 1981-01-03  18.8\n",
            "3 1981-01-04  14.6\n",
            "4 1981-01-05  15.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(year):\n",
        "  split_date = pd.datetime(year,1,1)\n",
        "  train_data = melbourne_data.loc[melbourne_data['Date'] < split_date]\n",
        "  test_data = melbourne_data.loc[melbourne_data['Date'] >= split_date]\n",
        "  scaler = MinMaxScaler()\n",
        "  train_data[[\"Temp\"]] = scaler.fit_transform(train_data[[\"Temp\"]] )\n",
        "  test_data[[\"Temp\"]] = scaler.fit_transform(test_data[[\"Temp\"]] )\n",
        "  train_data.set_index('Date', inplace=True)\n",
        "  test_data.set_index('Date', inplace=True)\n",
        "  return train_data, test_data"
      ],
      "metadata": {
        "id": "xmwke7MQe3rk"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, LSTM, Dense, Flatten\n",
        "from keras.models import Model\n",
        "\n",
        "def create_model(history_days, horizon_days):\n",
        "  model_input = Input(shape=(history_days, 1), name='x', dtype='float32')\n",
        "  z = LSTM(64, activation='relu', return_sequences=True)(model_input)\n",
        "  z = Flatten()(z) \n",
        "  z = Dense(horizon_days, activation='linear')(z)\n",
        "\n",
        "  # Keras model\n",
        "  model_keras = Model(inputs=model_input, outputs=z)\n",
        "\n",
        "  return model_keras\n",
        "\n",
        "model_3m = create_model(90, 30)\n",
        "model_6m = create_model(180, 30)\n",
        "model_12m = create_model(365, 30)"
      ],
      "metadata": {
        "id": "eGRpTlZ0e6mx"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data = split_data(1987)\n",
        "X_train, Y_train = create_training_points(train_data, history_days=90, horizon_days=30)\n",
        "model_3m.compile(optimizer='adam', loss='mse')\n",
        "model_3m.fit(X_train, Y_train, epochs=32, batch_size=32, verbose=1)"
      ],
      "metadata": {
        "id": "HzDOM0-4e9Hy",
        "outputId": "29ee7d82-e9c9-4bf4-9dc0-4a054a4c7869",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/32\n",
            "61/61 [==============================] - 4s 41ms/step - loss: 0.0294\n",
            "Epoch 2/32\n",
            "61/61 [==============================] - 3s 42ms/step - loss: 0.0139\n",
            "Epoch 3/32\n",
            "61/61 [==============================] - 2s 41ms/step - loss: 0.0130\n",
            "Epoch 4/32\n",
            "61/61 [==============================] - 3s 42ms/step - loss: 0.0128\n",
            "Epoch 5/32\n",
            "61/61 [==============================] - 3s 51ms/step - loss: 0.0124\n",
            "Epoch 6/32\n",
            "61/61 [==============================] - 3s 41ms/step - loss: 0.0123\n",
            "Epoch 7/32\n",
            "61/61 [==============================] - 3s 41ms/step - loss: 0.0122\n",
            "Epoch 8/32\n",
            "61/61 [==============================] - 3s 42ms/step - loss: 0.0122\n",
            "Epoch 9/32\n",
            "61/61 [==============================] - 3s 41ms/step - loss: 0.0120\n",
            "Epoch 10/32\n",
            "61/61 [==============================] - 3s 41ms/step - loss: 0.0120\n",
            "Epoch 11/32\n",
            "61/61 [==============================] - 2s 41ms/step - loss: 0.0121\n",
            "Epoch 12/32\n",
            "61/61 [==============================] - 3s 41ms/step - loss: 0.0119\n",
            "Epoch 13/32\n",
            "61/61 [==============================] - 3s 41ms/step - loss: 0.0118\n",
            "Epoch 14/32\n",
            "61/61 [==============================] - 3s 41ms/step - loss: 0.0118\n",
            "Epoch 15/32\n",
            "61/61 [==============================] - 3s 42ms/step - loss: 0.0117\n",
            "Epoch 16/32\n",
            "61/61 [==============================] - 3s 43ms/step - loss: 0.0118\n",
            "Epoch 17/32\n",
            "61/61 [==============================] - 3s 41ms/step - loss: 0.0115\n",
            "Epoch 18/32\n",
            "61/61 [==============================] - 2s 41ms/step - loss: 0.0115\n",
            "Epoch 19/32\n",
            "61/61 [==============================] - 2s 40ms/step - loss: 0.0115\n",
            "Epoch 20/32\n",
            "61/61 [==============================] - 3s 41ms/step - loss: 0.0114\n",
            "Epoch 21/32\n",
            "61/61 [==============================] - 3s 41ms/step - loss: 0.0114\n",
            "Epoch 22/32\n",
            "61/61 [==============================] - 2s 40ms/step - loss: 0.0115\n",
            "Epoch 23/32\n",
            "61/61 [==============================] - 2s 41ms/step - loss: 0.0115\n",
            "Epoch 24/32\n",
            "61/61 [==============================] - 3s 42ms/step - loss: 0.0112\n",
            "Epoch 25/32\n",
            "61/61 [==============================] - 2s 40ms/step - loss: 0.0111\n",
            "Epoch 26/32\n",
            "61/61 [==============================] - 2s 41ms/step - loss: 0.0111\n",
            "Epoch 27/32\n",
            "61/61 [==============================] - 2s 40ms/step - loss: 0.0110\n",
            "Epoch 28/32\n",
            "61/61 [==============================] - 3s 42ms/step - loss: 0.0109\n",
            "Epoch 29/32\n",
            "61/61 [==============================] - 3s 41ms/step - loss: 0.0108\n",
            "Epoch 30/32\n",
            "61/61 [==============================] - 3s 41ms/step - loss: 0.0107\n",
            "Epoch 31/32\n",
            "61/61 [==============================] - 3s 42ms/step - loss: 0.0106\n",
            "Epoch 32/32\n",
            "61/61 [==============================] - 3s 42ms/step - loss: 0.0106\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe20f39f410>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data = split_data(1987)\n",
        "X_test, Y_test = create_training_points(test_data, history_days=90, horizon_days=30)\n",
        "y_pred_3m = model_3m.predict(X_test)\n",
        "print(\"1987, k=3: \", mean_squared_error(y_pred_3m[0], Y_test[0]))\n",
        "\n",
        "train_data, test_data = split_data(1988)\n",
        "X_test, Y_test = create_training_points(test_data, history_days=90, horizon_days=30)\n",
        "y_pred_3m = model_3m.predict(X_test)\n",
        "print(\"1988, k=3: \", mean_squared_error(y_pred_3m[0], Y_train[1]))\n",
        "\n",
        "train_data, test_data = split_data(1989)\n",
        "X_test, Y_test = create_training_points(test_data, history_days=90, horizon_days=30)\n",
        "y_pred_3m = model_3m.predict(X_test)\n",
        "print(\"1989, k=3: \", mean_squared_error(y_pred_3m[0], Y_train[2]))"
      ],
      "metadata": {
        "id": "sjG8jlgTd_qo",
        "outputId": "56d1901f-2cf1-4ec4-c660-c106e62b6244",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1987, k=3:  0.027015956995034316\n",
            "1988, k=3:  0.011407004267776893\n",
            "1989, k=3:  0.026176998032545997\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data = split_data(1987)\n",
        "X_train, Y_train = create_training_points(train_data, history_days=180, horizon_days=30)\n",
        "model_6m.compile(optimizer='adam', loss='mse')\n",
        "model_6m.fit(X_train, Y_train, epochs=32, batch_size=32, verbose=1)"
      ],
      "metadata": {
        "id": "2IvMXvNveEDa",
        "outputId": "8dde61ff-cf39-4196-b16f-8a7ae820d3d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/32\n",
            "56/56 [==============================] - 6s 81ms/step - loss: 0.0302\n",
            "Epoch 2/32\n",
            "56/56 [==============================] - 4s 80ms/step - loss: 0.0121\n",
            "Epoch 3/32\n",
            "56/56 [==============================] - 4s 79ms/step - loss: 0.0121\n",
            "Epoch 4/32\n",
            "56/56 [==============================] - 5s 81ms/step - loss: 0.0116\n",
            "Epoch 5/32\n",
            "56/56 [==============================] - 4s 80ms/step - loss: 0.0113\n",
            "Epoch 6/32\n",
            "56/56 [==============================] - 5s 82ms/step - loss: 0.0112\n",
            "Epoch 7/32\n",
            "56/56 [==============================] - 5s 81ms/step - loss: 0.0111\n",
            "Epoch 8/32\n",
            "56/56 [==============================] - 5s 88ms/step - loss: 0.0109\n",
            "Epoch 9/32\n",
            "56/56 [==============================] - 4s 80ms/step - loss: 0.0107\n",
            "Epoch 10/32\n",
            "56/56 [==============================] - 5s 81ms/step - loss: 0.0106\n",
            "Epoch 11/32\n",
            "56/56 [==============================] - 4s 80ms/step - loss: 0.0106\n",
            "Epoch 12/32\n",
            "56/56 [==============================] - 4s 80ms/step - loss: 0.0105\n",
            "Epoch 13/32\n",
            "56/56 [==============================] - 4s 80ms/step - loss: 0.0106\n",
            "Epoch 14/32\n",
            "56/56 [==============================] - 4s 79ms/step - loss: 0.0104\n",
            "Epoch 15/32\n",
            "56/56 [==============================] - 5s 81ms/step - loss: 0.0103\n",
            "Epoch 16/32\n",
            "56/56 [==============================] - 4s 80ms/step - loss: 0.0102\n",
            "Epoch 17/32\n",
            "56/56 [==============================] - 4s 80ms/step - loss: 0.0102\n",
            "Epoch 18/32\n",
            "56/56 [==============================] - 4s 79ms/step - loss: 0.0100\n",
            "Epoch 19/32\n",
            "56/56 [==============================] - 5s 82ms/step - loss: 0.0099\n",
            "Epoch 20/32\n",
            "56/56 [==============================] - 5s 80ms/step - loss: 0.0099\n",
            "Epoch 21/32\n",
            "56/56 [==============================] - 4s 80ms/step - loss: 0.0098\n",
            "Epoch 22/32\n",
            "56/56 [==============================] - 4s 79ms/step - loss: 0.0097\n",
            "Epoch 23/32\n",
            "56/56 [==============================] - 4s 80ms/step - loss: 0.0096\n",
            "Epoch 24/32\n",
            "56/56 [==============================] - 5s 81ms/step - loss: 0.0094\n",
            "Epoch 25/32\n",
            "56/56 [==============================] - 4s 79ms/step - loss: 0.0094\n",
            "Epoch 26/32\n",
            "56/56 [==============================] - 4s 79ms/step - loss: 0.0092\n",
            "Epoch 27/32\n",
            "56/56 [==============================] - 4s 79ms/step - loss: 0.0091\n",
            "Epoch 28/32\n",
            "56/56 [==============================] - 5s 81ms/step - loss: 0.0091\n",
            "Epoch 29/32\n",
            "56/56 [==============================] - 4s 80ms/step - loss: 0.0089\n",
            "Epoch 30/32\n",
            "56/56 [==============================] - 4s 80ms/step - loss: 0.0088\n",
            "Epoch 31/32\n",
            "56/56 [==============================] - 4s 79ms/step - loss: 0.0087\n",
            "Epoch 32/32\n",
            "56/56 [==============================] - 4s 80ms/step - loss: 0.0086\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe20a22a810>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data = split_data(1987)\n",
        "X_test, Y_test = create_training_points(test_data, history_days=180, horizon_days=30)\n",
        "y_pred_6m = model_6m.predict(X_test)\n",
        "print(\"1987, k=6: \", mean_squared_error(y_pred_6m[0], Y_test[0]))\n",
        "\n",
        "train_data, test_data = split_data(1988)\n",
        "X_test, Y_test = create_training_points(test_data, history_days=180, horizon_days=30)\n",
        "y_pred_6m = model_6m.predict(X_test)\n",
        "print(\"1988, k=6: \", mean_squared_error(y_pred_6m[0], Y_train[1]))\n",
        "\n",
        "train_data, test_data = split_data(1989)\n",
        "X_test, Y_test = create_training_points(test_data, history_days=180, horizon_days=30)\n",
        "y_pred_6m = model_6m.predict(X_test)\n",
        "print(\"1989, k=6: \", mean_squared_error(y_pred_6m[0], Y_train[2]))"
      ],
      "metadata": {
        "id": "UmpY8CWbeL_C",
        "outputId": "b091871b-0daf-4204-f11d-34636078a5e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1987, k=6:  0.008685744760261092\n",
            "1988, k=6:  0.009408929715739653\n",
            "1989, k=6:  0.011483166456616822\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data = split_data(1987)\n",
        "X_train, Y_train = create_training_points(train_data, history_days=365, horizon_days=30)\n",
        "model_12m.compile(optimizer='adam', loss='mse')\n",
        "model_12m.fit(X_train, Y_train, epochs=32, batch_size=32, verbose=1)"
      ],
      "metadata": {
        "id": "HP-IHVQ8ePfW",
        "outputId": "5c4d4297-264d-4402-e92d-2e7a83f26fe9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/32\n",
            "44/44 [==============================] - 8s 158ms/step - loss: 0.0258\n",
            "Epoch 2/32\n",
            "44/44 [==============================] - 7s 157ms/step - loss: 0.0120\n",
            "Epoch 3/32\n",
            "44/44 [==============================] - 7s 160ms/step - loss: 0.0115\n",
            "Epoch 4/32\n",
            "44/44 [==============================] - 7s 157ms/step - loss: 0.0110\n",
            "Epoch 5/32\n",
            "44/44 [==============================] - 7s 158ms/step - loss: 0.0107\n",
            "Epoch 6/32\n",
            "44/44 [==============================] - 7s 164ms/step - loss: 0.0103\n",
            "Epoch 7/32\n",
            "44/44 [==============================] - 7s 160ms/step - loss: 0.0100\n",
            "Epoch 8/32\n",
            "44/44 [==============================] - 7s 159ms/step - loss: 0.0099\n",
            "Epoch 9/32\n",
            "44/44 [==============================] - 7s 161ms/step - loss: 0.0096\n",
            "Epoch 10/32\n",
            "44/44 [==============================] - 7s 160ms/step - loss: 0.0094\n",
            "Epoch 11/32\n",
            "44/44 [==============================] - 7s 161ms/step - loss: 0.0091\n",
            "Epoch 12/32\n",
            "44/44 [==============================] - 7s 159ms/step - loss: 0.0089\n",
            "Epoch 13/32\n",
            "44/44 [==============================] - 7s 160ms/step - loss: 0.0088\n",
            "Epoch 14/32\n",
            "44/44 [==============================] - 7s 159ms/step - loss: 0.0086\n",
            "Epoch 15/32\n",
            "44/44 [==============================] - 7s 158ms/step - loss: 0.0084\n",
            "Epoch 16/32\n",
            "44/44 [==============================] - 7s 160ms/step - loss: 0.0081\n",
            "Epoch 17/32\n",
            "44/44 [==============================] - 7s 158ms/step - loss: 0.0081\n",
            "Epoch 18/32\n",
            "44/44 [==============================] - 7s 159ms/step - loss: 0.0079\n",
            "Epoch 19/32\n",
            "44/44 [==============================] - 7s 161ms/step - loss: 0.0077\n",
            "Epoch 20/32\n",
            "44/44 [==============================] - 7s 161ms/step - loss: 0.0075\n",
            "Epoch 21/32\n",
            "44/44 [==============================] - 7s 158ms/step - loss: 0.0074\n",
            "Epoch 22/32\n",
            "44/44 [==============================] - 7s 159ms/step - loss: 0.0072\n",
            "Epoch 23/32\n",
            "44/44 [==============================] - 7s 159ms/step - loss: 0.0070\n",
            "Epoch 24/32\n",
            "44/44 [==============================] - 7s 162ms/step - loss: 0.0069\n",
            "Epoch 25/32\n",
            "44/44 [==============================] - 7s 158ms/step - loss: 0.0067\n",
            "Epoch 26/32\n",
            "44/44 [==============================] - 7s 159ms/step - loss: 0.0065\n",
            "Epoch 27/32\n",
            "44/44 [==============================] - 7s 159ms/step - loss: 0.0064\n",
            "Epoch 28/32\n",
            "44/44 [==============================] - 7s 158ms/step - loss: 0.0062\n",
            "Epoch 29/32\n",
            "44/44 [==============================] - 7s 163ms/step - loss: 0.0059\n",
            "Epoch 30/32\n",
            "44/44 [==============================] - 7s 160ms/step - loss: 0.0058\n",
            "Epoch 31/32\n",
            "44/44 [==============================] - 7s 158ms/step - loss: 0.0056\n",
            "Epoch 32/32\n",
            "44/44 [==============================] - 7s 161ms/step - loss: 0.0055\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe20a1d4290>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data = split_data(1987)\n",
        "X_test, Y_test = create_training_points(test_data, history_days=365, horizon_days=30)\n",
        "y_pred_12m = model_12m.predict(X_test)\n",
        "print(\"1987, k=12: \", mean_squared_error(y_pred_12m[0], Y_test[0]))\n",
        "\n",
        "train_data, test_data = split_data(1988)\n",
        "X_test, Y_test = create_training_points(test_data, history_days=365, horizon_days=30)\n",
        "y_pred_12m = model_12m.predict(X_test)\n",
        "print(\"1988, k=12: \", mean_squared_error(y_pred_12m[0], Y_train[1]))\n",
        "\n",
        "train_data, test_data = split_data(1989)\n",
        "X_test, Y_test = create_training_points(test_data, history_days=365, horizon_days=30)\n",
        "y_pred_12m = model_12m.predict(X_test)\n",
        "print(\"1989, k=12: \", mean_squared_error(y_pred_12m[0], Y_train[2]))"
      ],
      "metadata": {
        "id": "qfRKjjgfeQvM",
        "outputId": "e8faddf9-fe0f-4d85-b24c-9d9381b48590",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1987, k=12:  0.038549122123843024\n",
            "1988, k=12:  0.023159186670343206\n",
            "1989, k=12:  0.053148860747176015\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1987, k=12:  0.03865884223545268\n",
        "1988, k=12:  0.01985055503939519\n",
        "1989, k=12:  0.04430945823737449\n",
        "\n",
        "Metrics: Mean Squared Error\n",
        "\n",
        "1987, k=3: 0.027015956995034316 1988, k=3: 0.011407004267776893 1989, k=3: 0.026176998032545997\n",
        "\n",
        "1987, k=6: 0.008685744760261092 1988, k=6: 0.009408929715739653 1989, k=6: 0.011483166456616822\n",
        "\n",
        "1987, k=12: 0.038549122123843024 1988, k=12: 0.023159186670343206 1989, k=12: 0.053148860747176015\n"
      ],
      "metadata": {
        "id": "3snwFqadl4_Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le model le plus précis est le model est k=6.\n"
      ],
      "metadata": {
        "id": "wRVwqjuTl_6V"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Bienvenue_dans_Colaboratory-5.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}